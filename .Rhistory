tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[dateIndex[i]: length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
View(df)
df <- df[is.na(df$action) == FALSE,]
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1]], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[dateIndex[i]: length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
scrapeAction(urls[2])
z <- scrapeAction(urls[2])
View(z)
scrape <- lapply(urls, scrapeAction)
df <- data.table::rbindlist(scrape)
View(df)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1]], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[dateIndex[i] + 1: length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
scrape <- lapply(urls[1:10], scrapeAction)
df <- data.table::rbindlist(scrape)
View(df)
url <- urls[1]
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1]], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[dateIndex[i] + 1: length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
View(df)
i <- 1
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1]], stringsAsFactors = FALSE)
View(tmpDf)
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1] - 1], stringsAsFactors = FALSE)
View(tmpDf)
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1]], stringsAsFactors = FALSE)
View(tmpDf)
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1] - 1], stringsAsFactors = FALSE)
View(tmpDf)
tmpDf <- data.frame(action = page[dateIndex[i] + 1:dateIndex[i + 1] - 2], stringsAsFactors = FALSE)
View(tmpDf)
dateIndex[i] + 1
dateIndex[i + 1] - 1
tmpDf <- data.frame(action = page[(dateIndex[i] + 1):(dateIndex[i + 1] - 1)], stringsAsFactors = FALSE)
View(tmpDf)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[(dateIndex[i] + 1):(dateIndex[i + 1] - 1)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[(dateIndex[i] + 1): length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
View(df)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[(dateIndex[i] + 1):(dateIndex[i + 1] - 1)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[(dateIndex[i] + 1): length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
scrape <- lapply(urls[1:10], scrapeAction)
df <- data.table::rbindlist(scrape)
View(df)
#---Scrape NHL trade moves from ESPN
library(rvest)
library(lubridate)
baseUrl <- 'http://www.espn.com/nhl/transactions/_/date/'
myDate <- as.Date('2001-10-01')
urlDates <- character()
while(myDate < Sys.Date()){
urlDates <- append(urlDates, format(myDate, '%Y%m%d'))
myDate <- myDate + days(7)
}
urls <- paste0(baseUrl, urlDates)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[(dateIndex[i] + 1):(dateIndex[i + 1] - 1)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[(dateIndex[i] + 1): length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
scrape <- lapply(urls[1:10], scrapeAction)
df <- data.table::rbindlist(scrape)
library(snowfall)
install.packages('snowfall')
library(snowfall)
sfInit()
sfStop()
parallel::detectCores()
sfExport('scrapeAction')
sfExport('scrapeAction')
sfStop()
sfInit()
sfStop()
sfInit(cores = 4)
sfinit?
?sfInit
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary('rvest')
sfLibrary(rvest)
scrape <- sfLapply(urls[1:10], scrapeAction)
df <- data.table::rbindlist(scrape)
sfStop()
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary(rvest)
scrape <- sfLapply(urls[1:100], scrapeAction)
sfStop()
df <- data.table::rbindlist(scrape)
#---Scrape NHL trade moves from ESPN
library(rvest)
library(lubridate)
library(snowfall)
baseUrl <- 'http://www.espn.com/nhl/transactions/_/date/'
myDate <- as.Date('2001-10-01')
urlDates <- character()
while(myDate < Sys.Date()){
urlDates <- append(urlDates, format(myDate, '%Y%m%d'))
myDate <- myDate + days(7)
}
urls <- paste0(baseUrl, urlDates)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr') %>%
html_text
#try to convert to date
dateVec <- unlist(lapply(page, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
tmpDf <- data.frame(action = page[(dateIndex[i] + 1):(dateIndex[i + 1] - 1)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
} else{
tmpDf <- data.frame(action =    page[(dateIndex[i] + 1): length(page)], stringsAsFactors = FALSE)
tmpDf$date <- tmpDate
}
listOut[[i]] <- tmpDf
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary(rvest)
scrape <- sfLapply(urls, scrapeAction)
sfStop()
df <- data.table::rbindlist(scrape)
write.csv(df, 'nhlTrades.csv', row.names = FALSE)
View(df)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
for(r in 1:length(dateRange)){
action <- html_text(page[dateRange[r]])
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(action, team, tmpDate, stringsAsFactors = FALSE)
}
tmpAction <- data.table::rbindlist(tmpAction)
listOut[[i]] <- tmpAction
}
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
#---Scrape NHL trade moves from ESPN
library(rvest)
library(lubridate)
library(snowfall)
baseUrl <- 'http://www.espn.com/nhl/transactions/_/date/'
myDate <- as.Date('2001-10-01')
urlDates <- character()
while(myDate < Sys.Date()){
urlDates <- append(urlDates, format(myDate, '%Y%m%d'))
myDate <- myDate + days(7)
}
urls <- paste0(baseUrl, urlDates)
scrapeAction(urls[1])
scrapeAction(urls[2])
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
for(r in 1:length(dateRange)){
action <- html_text(page[dateRange[r]])
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(action, team, tmpDate, stringsAsFactors = FALSE)
}
tmpAction <- data.table::rbindlist(tmpAction)
listOut[[i]] <- tmpAction
}
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary(rvest)
scrape <- sfLapply(urls, scrapeAction)
sfStop()
df <- data.table::rbindlist(scrape)
View(df)
x <- df[is.na(df$team),]
x <- df[is.na(df$team) == FALSE,]
df <- df[is.na(df$team) == FALSE,]
View(df)
url
url <- urls[1]
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
i <- 1
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
r <- 1
action <- html_text(page[dateRange[r]])
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(action, team, tmpDate, stringsAsFactors = FALSE)
tmpAction[[r]]
?strsplit
actionSplit <- strsplit(action, ';')
actionSplit
actionSplit <- unlist(strsplit(action, ';'))
actionSplit
actionSplit <- unlist(strsplit(action, '; '))
actionSplit
tmpAction[[r]] <- data.frame(actionSplit, team, tmpDate, stringsAsFactors = FALSE)
data.frame(actionSplit, team, tmpDate, stringsAsFactors = FALSE)
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
for(r in 1:length(dateRange)){
action <- html_text(page[dateRange[r]])
actionSplit <- unlist(strsplit(action, '; '))
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(actionSplit, team, tmpDate, stringsAsFactors = FALSE)
}
tmpAction <- data.table::rbindlist(tmpAction)
listOut[[i]] <- tmpAction
}
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
df
View(df)
url
#---Scrape NHL trade moves from ESPN
library(rvest)
library(lubridate)
library(snowfall)
baseUrl <- 'http://www.espn.com/nhl/transactions/_/date/'
myDate <- as.Date('2001-10-01')
urlDates <- character()
while(myDate < Sys.Date()){
urlDates <- append(urlDates, format(myDate, '%Y%m%d'))
myDate <- myDate + days(7)
}
urls <- paste0(baseUrl, urlDates)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
for(r in 1:length(dateRange)){
action <- html_text(page[dateRange[r]])
actionSplit <- unlist(strsplit(action, '; '))
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(actionSplit, team, tmpDate, stringsAsFactors = FALSE)
}
tmpAction <- data.table::rbindlist(tmpAction)
listOut[[i]] <- tmpAction
}
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary(rvest)
scrape <- sfLapply(urls, scrapeAction)
sfStop()
df <- data.table::rbindlist(scrape)
df <- df[is.na(df$team) == FALSE,]
write.csv(df, 'nhlTrades.csv', row.names = FALSE)
View(df)
#---Scrape NHL trade moves from ESPN
library(rvest)
library(lubridate)
library(snowfall)
baseUrl <- 'http://www.espn.com/nhl/transactions/_/date/'
myDate <- as.Date('2001-10-01')
urlDates <- character()
while(myDate < Sys.Date()){
urlDates <- append(urlDates, format(myDate, '%Y%m%d'))
myDate <- myDate + days(7)
}
urls <- paste0(baseUrl, urlDates)
scrapeAction <- function(url){
page <- read_html(url) %>%
html_nodes('tr')
text <- page %>%
html_text()
#try to convert to date
dateVec <- unlist(lapply(text, as.Date, '%A, %B %d, %Y'))
dateIndex <- which(is.na(dateVec) == FALSE)
listOut <- list()
for(i in 1:length(dateIndex)){
tmpDate <- as.Date(dateVec[dateIndex[i]], origin = '1970-01-01')
if(is.na(dateIndex[i + 1]) == FALSE){
dateRange <- seq(dateIndex[i]+1, dateIndex[i + 1] - 1)
tmpAction <- list()
for(r in 1:length(dateRange)){
action <- html_text(page[dateRange[r]])
team <- page[dateRange[r]] %>%
html_node('a') %>%
html_attr('href') %>%
gsub('.+/(.*)', '\\1', .) %>%
gsub('-', ' ', .)
tmpAction[[r]] <- data.frame(action, team, tmpDate, stringsAsFactors = FALSE)
}
tmpAction <- data.table::rbindlist(tmpAction)
listOut[[i]] <- tmpAction
}
}
df <- data.table::rbindlist(listOut)
df <- df[is.na(df$action) == FALSE,]
return(df)
}
sfInit(parallel = TRUE, cpus = 4)
sfExport('scrapeAction')
sfLibrary(rvest)
scrape <- sfLapply(urls, scrapeAction)
sfStop()
df <- data.table::rbindlist(scrape)
df <- df[is.na(df$team) == FALSE,]
write.csv(df, 'nhlTrades.csv', row.names = FALSE)
View(df)
setwd("~/Desktop/sigmaNet")
pkgdown::build_site()
install.packages(pkgdown)
install.packages('pkgdown')
